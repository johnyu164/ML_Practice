{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dd_data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[pd.isnull(df['sentences_1000_str'])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['content'].str.len()<30].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['sentences_1000_str'])\n",
    "sequences = tokenizer.texts_to_sequences(df['sentences_1000_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (96393, 1000)\n",
      "Shape of label tensor: (96393, 14)\n"
     ]
    }
   ],
   "source": [
    "all_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(df['class_no']))\n",
    "print('Shape of data tensor:', all_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_val,y_train,y_val = train_test_split(all_data,labels,test_size=0.3,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67475, 1000), (67475, 14))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 200)         139369800 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 250)          150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 332, 250)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 83000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               16600200  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 156,123,064\n",
      "Trainable params: 156,123,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 139369800 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 139369800 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67475 samples, validate on 28918 samples\n",
      "Epoch 1/5\n",
      "67475/67475 [==============================] - 57s 848us/step - loss: 0.1182 - acc: 0.9558 - val_loss: 0.0868 - val_acc: 0.9638\n",
      "Epoch 2/5\n",
      "67475/67475 [==============================] - 53s 790us/step - loss: 0.0701 - acc: 0.9695 - val_loss: 0.0860 - val_acc: 0.9628\n",
      "Epoch 3/5\n",
      "67475/67475 [==============================] - 53s 790us/step - loss: 0.0566 - acc: 0.9745 - val_loss: 0.0916 - val_acc: 0.9602\n",
      "Epoch 4/5\n",
      "67475/67475 [==============================] - 53s 791us/step - loss: 0.0475 - acc: 0.9777 - val_loss: 0.0998 - val_acc: 0.9593\n",
      "Epoch 5/5\n",
      "67475/67475 [==============================] - 53s 788us/step - loss: 0.0425 - acc: 0.9796 - val_loss: 0.1079 - val_acc: 0.9574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e7fa037b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=5, verbose=1, validation_data=(x_val, y_val))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(pad_sequences(tokenizer.texts_to_sequences([df['sentences_1000_str'][1],]),MAX_SEQUENCE_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions>=0.5] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions<0.5] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'戴德 梁行 董事 總經理 顏炳立 認為 年 房市 落底 估計 年 落底 後 再 盤整 年 整體 走勢 如同 碟子 型 年 逐步 回彈 近期 北 市 商辦 市場 成為 資金 標的 台灣 電力公司 今天 舉行 北部 儲運 中心 南港 舊址 南側 土地 更案 招商 說明會 戴德 梁行 邀集 相關 廠商 舉行 開發 座談會 顏炳立 負責 分析 房市 走勢 顏炳立 表示 年 房市 斷 買單 風 一年 年 房市 落底 前 坡 相當 陡 利 再 價再降 量 再 縮 一年 預售 市場 量 再 縮 價再 破 北 市 蛋黃 區 房價 不黃 蛋白 區則 變黑 顏炳立 指出 年 房市 買賣 移轉 棟 數只 萬棟 年 回到 萬棟 看似 量 增 認為 房市 未 真正 落底 估計 年 落底 底部 盤整 年 年 再 如同 碟子 形狀 仍要 價格 買氣 成交量 相互 搭配 顏炳立 認為 年 商用 級 辦公大樓 缺貨 空置率 下降 引發 租金 房價 地價 上揚 優質 土地 難尋 以台電 釋出 南港區 坪 土地 準備 興建 住宅 商辦 滿足 部分 市場 資金 去處 主要 南港區 北 市府 東區 門戶 計畫 重要 大門 南港區 商用 土地 蓄勢待發 可望 成為 資金 追逐 標的 建議 最近 地主 讓利 降價 土地 市場 買賣 加溫 跡象 建商 努力 標購 存地 建商 應 慎用 子彈 備糧 風 戴德 梁行 董事 總經理 顏炳立 認為 年 房市 落底 估計 年 落底 後 再 盤整 年 整體 走勢 如同 碟子 型 年 逐步 回彈 近期 北 市 商辦 市場 成為 資金 標的 聯合報系 資料 記者 毛洪霖 攝影 文章 來源 udn 文章 標籤 房市 房市 落底 創作者 介紹 amy010203 房產 明星'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentences_1000_str'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(pad_sequences(tokenizer.texts_to_sequences([df['sentences_1000_str'][60000],]),MAX_SEQUENCE_LENGTH))\n",
    "predictions[predictions>=0.5] = 1\n",
    "predictions[predictions<0.5] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分享 分享 留言 列印 聯合 新聞網 Lucas LINE 傳送 關鍵字 FordGT ‧ Ford ‧ 跑車 擁有 一輛 FordGT rel gt FordGT 超級 跑車 熱血 車迷 福特 鐵粉 夢想 每年 輛 生產量 人 有錢 買不到 而本 週末 將會 有個 入主 FordGT 機會 先 準備 一大筆錢 飛去 美國 競標 MecumAuctions 提供 facebook 搭載 銀色 車身 烤 漆 車頭 延伸 車尾 黑色 飾條 這輛 年式 FordGT 超級 跑車 去年 出廠 輛中 輛 車款 配置 高光澤 碳纖維 套件 銀色 鍛造 輪圈 含 鈦合金 輪圈 螺帽 銀色 煞車 卡鉗 黑色 座艙 車輛 總行 駛 里程 英里 約 公里 上路 確實 買家 心動 MecumAuctions 提供 facebook 美國 研發 並在 加拿大 生產 FordGT 超級 跑車 搭載 爆發 匹 最大 馬力 公斤 米巔 峰值 扭力 輸出 升 雙渦輪 增壓 V6 汽油 引擎 搭配 速雙 離合器 變速箱 系統 後輪 驅動 設定 小時 公里 加速 僅需 秒 極速 達到 小時 公里 MecumAuctions 提供 facebook 想 買 一輛 FordGT 超級 跑車 已經 容易 總 里程 個位數 字 更是 難上加難 接受 銀色 FordGT 下定決心 買 週末 快趕 美國 印地安納 州 首府 印第安納波利斯 參加 MecumAuctions 舉辦 拍賣會 出價 最高者 能將 FordGT 開 回家 MecumAuctions 提供 facebookMecumAuctions 提供 facebookMecumAuctions 提供 facebookMecumAuctions 提供 facebook'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentences_1000_str'][60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
